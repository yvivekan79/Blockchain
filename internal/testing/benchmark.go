package testing

import (
        "context"
        "encoding/json"
        "fmt"
        "lscc-blockchain/config"
        "lscc-blockchain/internal/blockchain"
        "lscc-blockchain/internal/consensus"
        "lscc-blockchain/internal/utils"
        "lscc-blockchain/pkg/types"
        "math"
        "math/rand"
        "sort"
        "sync"
        "time"

        "github.com/sirupsen/logrus"
)

// BenchmarkSuite provides comprehensive performance testing for consensus algorithms
type BenchmarkSuite struct {
        config     *config.Config
        logger     *utils.Logger
        blockchain *blockchain.Blockchain
        results    map[string]*BenchmarkResult
        mu         sync.RWMutex
}

// BenchmarkConfig defines parameters for benchmark execution
type BenchmarkConfig struct {
        Algorithms          []string  `json:"algorithms"`
        ValidatorCounts     []int     `json:"validator_counts"`
        TransactionCounts   []int     `json:"transaction_counts"`
        CrossShardRatios    []float64 `json:"cross_shard_ratios"`
        ByzantineRatios     []float64 `json:"byzantine_ratios"`
        Duration            int       `json:"duration_seconds"`
        Iterations          int       `json:"iterations"`
        ConcurrentTests     int       `json:"concurrent_tests"`
        StatisticalConfidence float64 `json:"statistical_confidence"`
}

// BenchmarkResult stores comprehensive performance metrics
type BenchmarkResult struct {
        Algorithm            string                 `json:"algorithm"`
        ValidatorCount       int                    `json:"validator_count"`
        TransactionCount     int                    `json:"transaction_count"`
        CrossShardRatio      float64                `json:"cross_shard_ratio"`
        ByzantineRatio       float64                `json:"byzantine_ratio"`
        Throughput           float64                `json:"throughput_tps"`
        AverageLatency       time.Duration          `json:"average_latency"`
        P50Latency           time.Duration          `json:"p50_latency"`
        P95Latency           time.Duration          `json:"p95_latency"`
        P99Latency           time.Duration          `json:"p99_latency"`
        MessageComplexity    int64                  `json:"message_complexity"`
        CPUUsage             float64                `json:"cpu_usage_percent"`
        MemoryUsage          int64                  `json:"memory_usage_bytes"`
        NetworkBandwidth     int64                  `json:"network_bandwidth_bytes"`
        SuccessRate          float64                `json:"success_rate"`
        ConsensusRounds      int                    `json:"consensus_rounds"`
        CrossShardTxCount    int                    `json:"cross_shard_tx_count"`
        FailedTransactions   int                    `json:"failed_transactions"`
        SecurityMetrics      map[string]interface{} `json:"security_metrics"`
        PerformanceMetrics   map[string]float64     `json:"performance_metrics"`
        RawLatencies         []time.Duration        `json:"-"` // Not serialized
        Timestamp            time.Time              `json:"timestamp"`
        TestID               string                 `json:"test_id"`
}

// StatisticalSummary provides statistical analysis of benchmark results
type StatisticalSummary struct {
        Mean               float64 `json:"mean"`
        Median             float64 `json:"median"`
        StandardDeviation  float64 `json:"standard_deviation"`
        ConfidenceInterval struct {
                Lower float64 `json:"lower"`
                Upper float64 `json:"upper"`
                Level float64 `json:"confidence_level"`
        } `json:"confidence_interval"`
        SampleSize int `json:"sample_size"`
}

// NewBenchmarkSuite creates a new benchmark testing suite
func NewBenchmarkSuite(cfg *config.Config, logger *utils.Logger, bc *blockchain.Blockchain) *BenchmarkSuite {
        return &BenchmarkSuite{
                config:     cfg,
                logger:     logger,
                blockchain: bc,
                results:    make(map[string]*BenchmarkResult),
        }
}

// RunComprehensiveBenchmark executes a full benchmark suite comparing all algorithms
func (bs *BenchmarkSuite) RunComprehensiveBenchmark(benchConfig *BenchmarkConfig) (map[string][]*BenchmarkResult, error) {
        bs.logger.Info("Starting comprehensive benchmark suite", logrus.Fields{
                "algorithms":       benchConfig.Algorithms,
                "validator_counts": benchConfig.ValidatorCounts,
                "iterations":       benchConfig.Iterations,
                "timestamp":        time.Now().UTC(),
        })

        allResults := make(map[string][]*BenchmarkResult)
        
        // Test each algorithm
        for _, algorithm := range benchConfig.Algorithms {
                bs.logger.Info("Testing algorithm", logrus.Fields{
                        "algorithm": algorithm,
                        "timestamp": time.Now().UTC(),
                })
                
                algorithmResults := []*BenchmarkResult{}
                
                // Test different validator counts
                for _, validatorCount := range benchConfig.ValidatorCounts {
                        // Test different transaction loads
                        for _, transactionCount := range benchConfig.TransactionCounts {
                                // Test different cross-shard ratios
                                for _, crossShardRatio := range benchConfig.CrossShardRatios {
                                        // Test different Byzantine ratios
                                        for _, byzantineRatio := range benchConfig.ByzantineRatios {
                                                // Run multiple iterations for statistical significance
                                                iterationResults := []*BenchmarkResult{}
                                                
                                                for i := 0; i < benchConfig.Iterations; i++ {
                                                        result, err := bs.RunSingleBenchmark(&SingleBenchmarkConfig{
                                                                Algorithm:        algorithm,
                                                                ValidatorCount:   validatorCount,
                                                                TransactionCount: transactionCount,
                                                                CrossShardRatio:  crossShardRatio,
                                                                ByzantineRatio:   byzantineRatio,
                                                                Duration:         benchConfig.Duration,
                                                                TestID:          fmt.Sprintf("%s_%d_%d_%.2f_%.2f_%d", algorithm, validatorCount, transactionCount, crossShardRatio, byzantineRatio, i),
                                                        })
                                                        
                                                        if err != nil {
                                                                bs.logger.Error("Benchmark iteration failed", logrus.Fields{
                                                                        "algorithm":        algorithm,
                                                                        "validator_count":  validatorCount,
                                                                        "transaction_count": transactionCount,
                                                                        "iteration":        i,
                                                                        "error":           err,
                                                                        "timestamp":       time.Now().UTC(),
                                                                })
                                                                continue
                                                        }
                                                        
                                                        iterationResults = append(iterationResults, result)
                                                }
                                                
                                                // Calculate statistical summary for this configuration
                                                if len(iterationResults) > 0 {
                                                        summary := bs.calculateStatisticalSummary(iterationResults, benchConfig.StatisticalConfidence)
                                                        
                                                        // Create aggregate result
                                                        aggregateResult := bs.createAggregateResult(iterationResults, summary)
                                                        algorithmResults = append(algorithmResults, aggregateResult)
                                                }
                                        }
                                }
                        }
                }
                
                allResults[algorithm] = algorithmResults
        }
        
        // Generate comparative analysis
        bs.generateComparativeAnalysis(allResults)
        
        bs.logger.Info("Comprehensive benchmark completed", logrus.Fields{
                "total_tests": len(allResults),
                "timestamp":   time.Now().UTC(),
        })
        
        return allResults, nil
}

// SingleBenchmarkConfig defines parameters for a single benchmark test
type SingleBenchmarkConfig struct {
        Algorithm        string  `json:"algorithm"`
        ValidatorCount   int     `json:"validator_count"`
        TransactionCount int     `json:"transaction_count"`
        CrossShardRatio  float64 `json:"cross_shard_ratio"`
        ByzantineRatio   float64 `json:"byzantine_ratio"`
        Duration         int     `json:"duration_seconds"`
        TestID           string  `json:"test_id"`
}

// RunSingleBenchmark executes a single benchmark test
func (bs *BenchmarkSuite) RunSingleBenchmark(config *SingleBenchmarkConfig) (*BenchmarkResult, error) {
        startTime := time.Now()
        
        bs.logger.Info("Starting single benchmark", logrus.Fields{
                "algorithm":         config.Algorithm,
                "validator_count":   config.ValidatorCount,
                "transaction_count": config.TransactionCount,
                "cross_shard_ratio": config.CrossShardRatio,
                "byzantine_ratio":   config.ByzantineRatio,
                "test_id":          config.TestID,
                "timestamp":        startTime,
        })

        // Initialize consensus algorithm
        consensusEngine, err := bs.initializeConsensus(config.Algorithm, config.ValidatorCount, config.ByzantineRatio)
        if err != nil {
                return nil, fmt.Errorf("failed to initialize consensus: %w", err)
        }

        // Generate test transactions
        transactions := bs.generateTestTransactions(config.TransactionCount, config.CrossShardRatio)
        
        // Initialize metrics collection
        result := &BenchmarkResult{
                Algorithm:        config.Algorithm,
                ValidatorCount:   config.ValidatorCount,
                TransactionCount: config.TransactionCount,
                CrossShardRatio:  config.CrossShardRatio,
                ByzantineRatio:   config.ByzantineRatio,
                TestID:          config.TestID,
                Timestamp:       startTime,
                SecurityMetrics: make(map[string]interface{}),
                PerformanceMetrics: make(map[string]float64),
                RawLatencies:    make([]time.Duration, 0),
        }

        // Run benchmark test
        ctx, cancel := context.WithTimeout(context.Background(), time.Duration(config.Duration)*time.Second)
        defer cancel()

        successfulTx := 0
        failedTx := 0
        totalLatency := time.Duration(0)
        messageCount := int64(0)
        
        // Process transactions and measure performance
        for i, tx := range transactions {
                txStartTime := time.Now()
                
                // Process transaction through consensus
                success, messages, err := bs.processTransactionWithConsensus(consensusEngine, tx)
                
                txLatency := time.Since(txStartTime)
                result.RawLatencies = append(result.RawLatencies, txLatency)
                totalLatency += txLatency
                messageCount += messages
                
                if err != nil || !success {
                        failedTx++
                        bs.logger.Debug("Transaction failed", logrus.Fields{
                                "tx_id":     tx.ID,
                                "error":     err,
                                "timestamp": time.Now().UTC(),
                        })
                } else {
                        successfulTx++
                }
                
                // Check context timeout
                if ctx.Err() != nil {
                        bs.logger.Warn("Benchmark timeout reached", logrus.Fields{
                                "processed_transactions": i + 1,
                                "total_transactions":    len(transactions),
                        })
                        break
                }
        }

        totalDuration := time.Since(startTime)
        
        // Calculate performance metrics
        result.Throughput = float64(successfulTx) / totalDuration.Seconds()
        result.AverageLatency = totalLatency / time.Duration(len(result.RawLatencies))
        result.MessageComplexity = messageCount
        result.SuccessRate = float64(successfulTx) / float64(successfulTx+failedTx)
        result.FailedTransactions = failedTx
        result.CrossShardTxCount = int(float64(config.TransactionCount) * config.CrossShardRatio)
        
        // Calculate latency percentiles
        bs.calculateLatencyPercentiles(result)
        
        // Collect system metrics
        bs.collectSystemMetrics(result)
        
        // Calculate security metrics
        bs.calculateSecurityMetrics(result, consensusEngine)

        bs.logger.Info("Single benchmark completed", logrus.Fields{
                "algorithm":       config.Algorithm,
                "throughput":      result.Throughput,
                "average_latency": result.AverageLatency.Milliseconds(),
                "success_rate":    result.SuccessRate,
                "test_id":         config.TestID,
                "duration":        totalDuration.Seconds(),
                "timestamp":       time.Now().UTC(),
        })

        return result, nil
}

// initializeConsensus creates and configures a consensus engine for testing
func (bs *BenchmarkSuite) initializeConsensus(algorithm string, validatorCount int, byzantineRatio float64) (consensus.Consensus, error) {
        // Generate test validators (not used in current implementation)
        _ = bs.generateTestValidators(validatorCount, byzantineRatio)
        
        switch algorithm {
        case "pbft":
                return consensus.NewPBFT(bs.config, bs.logger)  
        case "ppbft":
                return consensus.NewPracticalPBFT(bs.config, bs.logger)
        case "pos":
                return consensus.NewPBFT(bs.config, bs.logger) // Use PBFT as PoS placeholder
        case "pow":
                return consensus.NewPBFT(bs.config, bs.logger) // Use PBFT as PoW placeholder
        case "lscc":
                return consensus.NewLSCC(bs.config, bs.logger)
        default:
                return nil, fmt.Errorf("unsupported consensus algorithm: %s", algorithm)
        }
}

// generateTestTransactions creates a set of test transactions with specified cross-shard ratio
func (bs *BenchmarkSuite) generateTestTransactions(count int, crossShardRatio float64) []*types.Transaction {
        transactions := make([]*types.Transaction, count)
        crossShardCount := int(float64(count) * crossShardRatio)
        
        for i := 0; i < count; i++ {
                _ = i < crossShardCount // Cross-shard flag (unused in current implementation)
                
                tx := &types.Transaction{
                        ID:        fmt.Sprintf("test_tx_%d_%d", time.Now().UnixNano(), i),
                        From:      bs.generateRandomAddress(),
                        To:        bs.generateRandomAddress(),
                        Amount:    int64(rand.Float64() * 1000), // Random amount up to 1000
                        Timestamp: time.Now().UTC(),
                        ShardID:   rand.Intn(3), // Random shard 0-2
                }
                
                // Generate transaction signature
                tx.Signature = utils.HashString(fmt.Sprintf("%s_%s_%.2f_%d", tx.From, tx.To, tx.Amount, tx.Timestamp.UnixNano()))
                
                transactions[i] = tx
        }
        
        return transactions
}

// generateTestValidators creates test validators with specified Byzantine ratio
func (bs *BenchmarkSuite) generateTestValidators(count int, byzantineRatio float64) []*types.Validator {
        validators := make([]*types.Validator, count)
        byzantineCount := int(float64(count) * byzantineRatio)
        
        for i := 0; i < count; i++ {
                _ = i < byzantineCount // Byzantine flag (unused in current implementation)
                
                validator := &types.Validator{
                        Address: bs.generateRandomAddress(),
                        Stake:   int64(rand.Float64() * 1000000), // Random stake
                }
                
                validators[i] = validator
        }
        
        return validators
}

// processTransactionWithConsensus processes a transaction through the consensus mechanism
func (bs *BenchmarkSuite) processTransactionWithConsensus(consensusEngine consensus.Consensus, tx *types.Transaction) (bool, int64, error) {
        // Create a block with the transaction
        block := &types.Block{
                Index:        int64(time.Now().UnixNano()),
                Timestamp:    time.Now().UTC(),
                Transactions: []*types.Transaction{tx},
                ShardID:      tx.ShardID,
                Hash:         utils.HashString(fmt.Sprintf("block_%s_%d", tx.ID, time.Now().UnixNano())),
        }
        
        // Track message count (simplified estimation)
        messageCount := int64(1) // Base message count
        
        // Process through consensus
        success, err := consensusEngine.ProcessBlock(block, bs.generateTestValidators(9, 0.1))
        
        // Estimate message complexity based on algorithm
        switch consensusEngine.(type) {
        case *consensus.PBFT:
                messageCount = int64(math.Pow(float64(9), 2)) // O(nÂ²) for PBFT
        case *consensus.LSCC:
                messageCount = int64(9 * math.Log2(9)) // O(n log n) for LSCC
        default:
                messageCount = int64(9) // O(n) for others
        }
        
        return success, messageCount, err
}

// calculateLatencyPercentiles calculates P50, P95, P99 latency percentiles
func (bs *BenchmarkSuite) calculateLatencyPercentiles(result *BenchmarkResult) {
        if len(result.RawLatencies) == 0 {
                return
        }
        
        // Sort latencies
        latencies := make([]time.Duration, len(result.RawLatencies))
        copy(latencies, result.RawLatencies)
        sort.Slice(latencies, func(i, j int) bool {
                return latencies[i] < latencies[j]
        })
        
        n := len(latencies)
        result.P50Latency = latencies[n*50/100]
        result.P95Latency = latencies[n*95/100]
        result.P99Latency = latencies[n*99/100]
}

// collectSystemMetrics collects CPU, memory, and network metrics
func (bs *BenchmarkSuite) collectSystemMetrics(result *BenchmarkResult) {
        // Simplified system metrics collection
        // In production, this would use actual system monitoring
        result.CPUUsage = 25.5 + rand.Float64()*50.0 // Simulated CPU usage
        result.MemoryUsage = int64(100*1024*1024 + rand.Int63n(500*1024*1024)) // Simulated memory usage
        result.NetworkBandwidth = int64(1024*1024 + rand.Int63n(10*1024*1024)) // Simulated network usage
}

// calculateSecurityMetrics evaluates security-related performance metrics
func (bs *BenchmarkSuite) calculateSecurityMetrics(result *BenchmarkResult, consensusEngine consensus.Consensus) {
        result.SecurityMetrics["byzantine_tolerance"] = float64(result.ValidatorCount) / 3.0
        result.SecurityMetrics["finality_probability"] = 0.999 // Simplified
        result.SecurityMetrics["attack_resistance_score"] = 0.95 // Simplified
        
        // Algorithm-specific security metrics
        switch consensusEngine.(type) {
        case *consensus.LSCC:
                result.SecurityMetrics["layer_redundancy"] = 3.0
                result.SecurityMetrics["cross_channel_verification"] = true
        case *consensus.PBFT:
                result.SecurityMetrics["view_changes"] = rand.Intn(5)
        }
}

// calculateStatisticalSummary computes statistical analysis of multiple benchmark runs
func (bs *BenchmarkSuite) calculateStatisticalSummary(results []*BenchmarkResult, confidenceLevel float64) *StatisticalSummary {
        if len(results) == 0 {
                return nil
        }
        
        // Extract throughput values for statistical analysis
        throughputs := make([]float64, len(results))
        for i, result := range results {
                throughputs[i] = result.Throughput
        }
        
        // Calculate mean
        sum := 0.0
        for _, value := range throughputs {
                sum += value
        }
        mean := sum / float64(len(throughputs))
        
        // Calculate standard deviation
        variance := 0.0
        for _, value := range throughputs {
                variance += math.Pow(value-mean, 2)
        }
        stdDev := math.Sqrt(variance / float64(len(throughputs)-1))
        
        // Calculate median
        sortedValues := make([]float64, len(throughputs))
        copy(sortedValues, throughputs)
        sort.Float64s(sortedValues)
        median := sortedValues[len(sortedValues)/2]
        
        // Calculate confidence interval (simplified t-distribution)
        tValue := 1.96 // Approximate for 95% confidence
        if confidenceLevel == 0.99 {
                tValue = 2.576
        }
        
        margin := tValue * (stdDev / math.Sqrt(float64(len(throughputs))))
        
        return &StatisticalSummary{
                Mean:              mean,
                Median:            median,
                StandardDeviation: stdDev,
                ConfidenceInterval: struct {
                        Lower float64 `json:"lower"`
                        Upper float64 `json:"upper"`
                        Level float64 `json:"confidence_level"`
                }{
                        Lower: mean - margin,
                        Upper: mean + margin,
                        Level: confidenceLevel,
                },
                SampleSize: len(results),
        }
}

// createAggregateResult creates a single result representing multiple iterations
func (bs *BenchmarkSuite) createAggregateResult(results []*BenchmarkResult, summary *StatisticalSummary) *BenchmarkResult {
        if len(results) == 0 {
                return nil
        }
        
        // Use first result as template
        aggregate := *results[0]
        aggregate.TestID = fmt.Sprintf("aggregate_%s", results[0].TestID)
        
        // Calculate aggregate metrics
        aggregate.Throughput = summary.Mean
        
        // Calculate average latency
        totalLatency := time.Duration(0)
        for _, result := range results {
                totalLatency += result.AverageLatency
        }
        aggregate.AverageLatency = totalLatency / time.Duration(len(results))
        
        // Calculate average success rate
        totalSuccessRate := 0.0
        for _, result := range results {
                totalSuccessRate += result.SuccessRate
        }
        aggregate.SuccessRate = totalSuccessRate / float64(len(results))
        
        // Add statistical metadata
        aggregate.PerformanceMetrics["statistical_mean"] = summary.Mean
        aggregate.PerformanceMetrics["statistical_stddev"] = summary.StandardDeviation
        aggregate.PerformanceMetrics["confidence_interval_lower"] = summary.ConfidenceInterval.Lower
        aggregate.PerformanceMetrics["confidence_interval_upper"] = summary.ConfidenceInterval.Upper
        aggregate.PerformanceMetrics["sample_size"] = float64(summary.SampleSize)
        
        return &aggregate
}

// generateComparativeAnalysis creates comparative analysis between algorithms
func (bs *BenchmarkSuite) generateComparativeAnalysis(results map[string][]*BenchmarkResult) {
        bs.logger.Info("Generating comparative analysis", logrus.Fields{
                "algorithms": len(results),
                "timestamp":  time.Now().UTC(),
        })
        
        // Find best performing algorithm for each metric
        bestThroughput := ""
        bestLatency := ""
        bestSuccessRate := ""
        
        maxThroughput := 0.0
        minLatency := time.Duration(math.MaxInt64)
        maxSuccessRate := 0.0
        
        for algorithm, algorithmResults := range results {
                if len(algorithmResults) == 0 {
                        continue
                }
                
                // Calculate average metrics for algorithm
                avgThroughput := 0.0
                avgLatency := time.Duration(0)
                avgSuccessRate := 0.0
                
                for _, result := range algorithmResults {
                        avgThroughput += result.Throughput
                        avgLatency += result.AverageLatency
                        avgSuccessRate += result.SuccessRate
                }
                
                avgThroughput /= float64(len(algorithmResults))
                avgLatency /= time.Duration(len(algorithmResults))
                avgSuccessRate /= float64(len(algorithmResults))
                
                // Check if this algorithm is best in any category
                if avgThroughput > maxThroughput {
                        maxThroughput = avgThroughput
                        bestThroughput = algorithm
                }
                
                if avgLatency < minLatency {
                        minLatency = avgLatency
                        bestLatency = algorithm
                }
                
                if avgSuccessRate > maxSuccessRate {
                        maxSuccessRate = avgSuccessRate
                        bestSuccessRate = algorithm
                }
        }
        
        bs.logger.Info("Comparative analysis results", logrus.Fields{
                "best_throughput":    bestThroughput,
                "max_throughput":     maxThroughput,
                "best_latency":       bestLatency,
                "min_latency_ms":     minLatency.Milliseconds(),
                "best_success_rate":  bestSuccessRate,
                "max_success_rate":   maxSuccessRate,
                "timestamp":          time.Now().UTC(),
        })
}

// generateRandomAddress creates a random blockchain address for testing
func (bs *BenchmarkSuite) generateRandomAddress() string {
        return fmt.Sprintf("test_addr_%d_%d", time.Now().UnixNano(), rand.Intn(10000))
}

// ExportResults exports benchmark results to JSON format
func (bs *BenchmarkSuite) ExportResults(results map[string][]*BenchmarkResult, filename string) error {
        data, err := json.MarshalIndent(results, "", "  ")
        if err != nil {
                return fmt.Errorf("failed to marshal results: %w", err)
        }
        
        // In a real implementation, this would write to file
        bs.logger.Info("Benchmark results exported", logrus.Fields{
                "filename":     filename,
                "data_size":    len(data),
                "timestamp":    time.Now().UTC(),
        })
        
        return nil
}